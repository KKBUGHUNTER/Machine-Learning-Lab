{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "1. **Define the Problem:**\n",
    "   - Clearly articulate the problem you want to solve. What is the objective of your project? What are you trying to achieve with machine learning?\n",
    "\n",
    "2. **Gather Data:**\n",
    "   - Collect relevant data for your problem. Ensure that the data is clean, well-organized, and represents the problem you're trying to solve.\n",
    "\n",
    "3. **Explore and Understand the Data:**\n",
    "   - Perform exploratory data analysis (EDA) to understand the characteristics of the data. Visualize data distributions, identify patterns, and check for missing values.\n",
    "\n",
    "4. **Data Preprocessing:**\n",
    "   - Handle missing data, encode categorical variables, and scale numerical features. This step is crucial to ensure that the data is suitable for training machine learning models.\n",
    "\n",
    "5. **Feature Engineering:**\n",
    "   - Create new features or transform existing ones to better represent the underlying patterns in the data. Feature engineering can significantly impact the performance of your models.\n",
    "\n",
    "6. **Split the Data:**\n",
    "   - Divide the dataset into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune hyperparameters, and the test set is used to evaluate the model's generalization performance.\n",
    "\n",
    "7. **Choose a Model:**\n",
    "   - Select a machine learning algorithm that is suitable for your problem. Consider the characteristics of your data and the nature of the task (classification, regression, clustering, etc.).\n",
    "\n",
    "8. **Train the Model:**\n",
    "   - Train your chosen model on the training data. Adjust hyperparameters based on the performance on the validation set to prevent overfitting.\n",
    "\n",
    "9. **Evaluate the Model:**\n",
    "   - Use the test set to assess the model's performance. Metrics such as accuracy, precision, recall, F1 score, or mean squared error can be used depending on the nature of the problem.\n",
    "\n",
    "10. **Hyperparameter Tuning:**\n",
    "    - Fine-tune the hyperparameters of your model to optimize its performance. Techniques like grid search or random search can be employed.\n",
    "\n",
    "11. **Deploy the Model:**\n",
    "    - If the model meets the desired performance, deploy it to a production environment. Ensure that it can handle real-world data and integrate it with the existing system.\n",
    "\n",
    "12. **Monitor and Maintain:**\n",
    "    - Regularly monitor the model's performance in a production environment. Update the model as needed and handle issues that may arise.\n",
    "\n",
    "13. **Document the Process:**\n",
    "    - Document every step of your project, including data sources, preprocessing steps, model architecture, and performance metrics. This documentation is valuable for reproducibility and collaboration.\n",
    "\n",
    "14. **Continuous Learning:**\n",
    "    - Stay updated with the latest advancements in machine learning. Continuous learning is crucial in the rapidly evolving field of ML.\n",
    "\n",
    "Remember, each project is unique, and flexibility is essential. Adjustments may be needed based on the specifics of your problem and the data you're working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-1 Define the Problem\n",
    "Clearly articulate the problem you want to solve. What is the objective of your project? What are you trying to achieve with machine learning? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-2 Gather Data\n",
    "Data Set \n",
    "- Loan Amount Approvel Prediction  -  https://www.kaggle.com/code/ashishkumarjayswal/loan-amount-approvel-prediction\n",
    "\n",
    "- MNIST Database of Handwritten Digits - https://archive.ics.uci.edu/dataset/683/mnist+database+of+handwritten+digits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-3 Explore and Understand the Data\n",
    "Exploring and understanding the data is a crucial step in any machine learning project. This phase helps you gain insights into the characteristics of the data, identify patterns, and make informed decisions about preprocessing. Here are the sub-tasks involved in exploring and understanding the data:\n",
    "\n",
    "1. **Descriptive Statistics:**\n",
    "   - Compute basic statistical measures such as mean, median, standard deviation, minimum, maximum, and quartiles for numerical features. This provides a high-level overview of the data distribution.\n",
    "\n",
    "2. **Data Visualization:**\n",
    "   - Create visualizations to better understand the data. Use histograms, box plots, scatter plots, and other relevant charts to visualize the distribution of features and relationships between variables.\n",
    "\n",
    "3. **Correlation Analysis:**\n",
    "   - Calculate correlation coefficients to identify relationships between numerical features. This helps understand which variables are positively or negatively correlated.\n",
    "\n",
    "4. **Categorical Feature Analysis:**\n",
    "   - Explore the distribution of categorical features using bar charts or pie charts. Understand the frequency of each category and identify potential patterns.\n",
    "\n",
    "5. **Target Variable Analysis:**\n",
    "   - If you have a supervised learning problem, analyze the distribution of the target variable. Understand the balance between classes in classification tasks and the distribution of values in regression tasks.\n",
    "\n",
    "6. **Outlier Detection:**\n",
    "   - Identify outliers in the data that may affect the model's performance. Visualize and analyze extreme values or use statistical methods to detect outliers.\n",
    "\n",
    "7. **Feature Distribution Analysis:**\n",
    "   - Analyze the distribution of each feature to understand its shape and potential impact on modeling. Consider transformations if the data distribution is skewed.\n",
    "\n",
    "8. **Feature Relationships:**\n",
    "   - Explore relationships between pairs of features to identify any patterns or dependencies. This can be done through scatter plots, pair plots, or correlation matrices.\n",
    "\n",
    "9. **Data Imbalance Check:**\n",
    "   - Check for class imbalances in classification problems. If there is a significant imbalance, it may impact the model's ability to generalize well to minority classes.\n",
    "\n",
    "10. **Time Series Analysis (if applicable):**\n",
    "    - If your data involves a temporal component, analyze trends, seasonality, and patterns over time. This is crucial for time series forecasting tasks.\n",
    "\n",
    "11. **Missing Values Analysis:**\n",
    "    - Investigate the presence of missing values in the dataset. Understand whether missing values occur randomly or if there's a pattern, and decide on an appropriate strategy for handling them.\n",
    "\n",
    "12. **Feature Scaling Analysis:**\n",
    "    - If your model is sensitive to the scale of features (e.g., in distance-based algorithms), analyze whether scaling is necessary. Common techniques include standardization or normalization.\n",
    "\n",
    "13. **Feature Importance Analysis (if applicable):**\n",
    "    - For predictive modeling, analyze the importance of different features. This can be done using techniques like feature importance plots or model-specific methods.\n",
    "\n",
    "14. **Hypothesis Testing (if applicable):**\n",
    "    - If you have specific hypotheses about the data, perform statistical tests to validate or refute them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-4 Data Preprocessing\n",
    "Data preprocessing is a critical step in preparing your data for machine learning models. Here are the sub-tasks involved in data preprocessing:\n",
    "\n",
    "1. **Handling Missing Data:**\n",
    "   - Identify and handle missing values. Options include removing rows with missing values, imputing with mean or median, or using more advanced imputation techniques.\n",
    "\n",
    "2. **Outlier Detection and Removal:**\n",
    "   - Identify and handle outliers that may negatively impact model performance. This can involve removing outliers or transforming the data to mitigate their influence.\n",
    "\n",
    "3. **Data Transformation:**\n",
    "   - Apply transformations to the data, such as log transformations, to address issues like skewness and make the distribution more suitable for modeling.\n",
    "\n",
    "4. **Encoding Categorical Variables:**\n",
    "   - Convert categorical variables into a format suitable for machine learning models. This may involve one-hot encoding, label encoding, or using other techniques based on the nature of the data and the model.\n",
    "\n",
    "5. **Scaling and Normalization:**\n",
    "   - Scale numerical features to a standard range. This is crucial for models that are sensitive to the scale of input features, such as distance-based algorithms.\n",
    "\n",
    "6. **Handling Imbalanced Classes:**\n",
    "   - If you have imbalanced classes in a classification problem, consider strategies such as oversampling the minority class, undersampling the majority class, or using synthetic data generation techniques.\n",
    "\n",
    "7. **Feature Engineering:**\n",
    "   - Create new features or modify existing ones to enhance the model's ability to capture patterns in the data. This might involve creating interaction terms, polynomial features, or aggregating information.\n",
    "\n",
    "8. **Binning or Discretization:**\n",
    "   - Convert continuous features into discrete bins. This can help capture non-linear relationships and patterns in the data.\n",
    "\n",
    "9. **Handling Skewed Data:**\n",
    "   - Address skewness in the distribution of numerical features. Techniques like log transformation or power transformation can be applied to make the data more symmetric.\n",
    "\n",
    "10. **Dealing with Duplicate Data:**\n",
    "    - Identify and remove duplicate rows in the dataset. Duplicate data can lead to overfitting and affect model performance.\n",
    "\n",
    "11. **Handling Time-Series Data (if applicable):**\n",
    "    - For time-series data, consider handling temporal aspects such as lag features, rolling statistics, or resampling to a consistent time frequency.\n",
    "\n",
    "12. **Data Splitting (Train-Validation-Test):**\n",
    "    - Split the dataset into training, validation, and test sets. The training set is used to train the model, the validation set helps tune hyperparameters, and the test set evaluates the model's generalization performance.\n",
    "\n",
    "13. **Feature Scaling Consistency:**\n",
    "    - If scaling is performed, ensure that it is consistent across training, validation, and test sets to prevent data leakage.\n",
    "\n",
    "14. **Handling Text Data (if applicable):**\n",
    "    - Process and tokenize text data. This might involve techniques like stemming, lemmatization, and converting text into numerical representations using methods like TF-IDF or word embeddings.\n",
    "\n",
    "15. **Normalization of Target Variable (if applicable):**\n",
    "    - If your target variable has a skewed distribution, consider normalizing it to improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-5 Feature Engineering\n",
    "Feature engineering involves creating new features or transforming existing ones to enhance the performance of machine learning models. Here are the sub-tasks involved in feature engineering:\n",
    "\n",
    "1. **Creation of Interaction Features:**\n",
    "   - Generate new features by combining existing features. For example, if you have features A and B, create a new feature A*B representing their interaction.\n",
    "\n",
    "2. **Polynomial Features:**\n",
    "   - Introduce polynomial features by raising existing features to higher degrees. This can capture non-linear relationships in the data.\n",
    "\n",
    "3. **Logarithmic and Exponential Transformations:**\n",
    "   - Apply logarithmic or exponential transformations to numerical features to handle skewed distributions and make the data more suitable for certain types of models.\n",
    "\n",
    "4. **Binning or Discretization:**\n",
    "   - Convert continuous features into discrete bins. This can help capture non-linear patterns and simplify complex relationships.\n",
    "\n",
    "5. **Handling Time-Related Features (if applicable):**\n",
    "   - Extract relevant time-related features, such as day of the week, month, or year, from timestamp data. This is important for time-series analysis.\n",
    "\n",
    "6. **Encoding Cyclical Features (if applicable):**\n",
    "   - For cyclical features like time of day or month, use techniques such as sine and cosine transformations to encode cyclic patterns.\n",
    "\n",
    "7. **Feature Scaling and Normalization:**\n",
    "   - Scale numerical features to a consistent range. This ensures that features with different scales do not disproportionately influence certain models.\n",
    "\n",
    "8. **Feature Crosses:**\n",
    "   - Combine two or more features in a non-linear way to capture specific interactions. This is particularly useful in models like neural networks.\n",
    "\n",
    "9. **Handling Missing Values (if applicable):**\n",
    "   - Create new features to indicate whether a value is missing in another feature. This can help models learn patterns related to missing data.\n",
    "\n",
    "10. **Text Embeddings (if applicable):**\n",
    "    - Convert text data into numerical representations using techniques like word embeddings (e.g., Word2Vec, GloVe) or TF-IDF.\n",
    "\n",
    "11. **Aggregation of Features:**\n",
    "    - Aggregate information by creating summary statistics (e.g., mean, median, sum) for groups of related features. This is useful when dealing with data with hierarchical structures.\n",
    "\n",
    "12. **Domain-Specific Feature Engineering:**\n",
    "    - Incorporate domain knowledge to create features that are specifically tailored to the problem at hand. This may involve incorporating external data or engineering features based on expert insights.\n",
    "\n",
    "13. **One-Hot Encoding (for Categorical Variables):**\n",
    "    - Convert categorical variables into binary vectors to make them suitable for models that require numerical input.\n",
    "\n",
    "14. **Feature Importance Analysis:**\n",
    "    - Use techniques like tree-based models to analyze feature importance and focus on the most relevant features for predictive modeling.\n",
    "\n",
    "15. **Handling High-Dimensional Data:**\n",
    "    - If dealing with high-dimensional data, consider dimensionality reduction techniques such as Principal Component Analysis (PCA) or feature selection methods.\n",
    "\n",
    "16. **Feature Scaling Consistency:**\n",
    "    - Ensure that feature scaling is consistent across training, validation, and test sets to prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step-7 Choose a Model\n",
    "Choosing a model is a crucial step in the machine learning process, and it involves several sub-tasks. Here are the sub-tasks involved in choosing a model:\n",
    "\n",
    "1. **Define the Problem Type:**\n",
    "   - Clearly understand whether your problem is a classification, regression, clustering, or another type of machine learning task. The nature of the problem will guide the selection of appropriate models.\n",
    "\n",
    "2. **Understand Model Requirements:**\n",
    "   - Identify specific requirements and constraints for your project, such as interpretability, scalability, real-time predictions, or model complexity. These requirements will help narrow down the choice of models.\n",
    "\n",
    "3. **Explore Different Model Families:**\n",
    "   - Familiarize yourself with various machine learning model families, including linear models, tree-based models, support vector machines, neural networks, ensemble methods, and more. Understand the strengths and weaknesses of each family.\n",
    "\n",
    "4. **Consider Model Complexity:**\n",
    "   - Assess the complexity of models and how well they align with the complexity of your problem. Choose a model that is complex enough to capture patterns but not overly complex to avoid overfitting.\n",
    "\n",
    "5. **Review Model Performance Metrics:**\n",
    "   - Understand the evaluation metrics suitable for your specific problem. For example, accuracy, precision, recall, F1 score for classification, or mean squared error for regression. Different models may excel in different metrics.\n",
    "\n",
    "6. **Experiment with Baseline Models:**\n",
    "   - Start with simple, interpretable models as baseline models. This provides a benchmark for more complex models and helps you assess whether the added complexity is justified by improved performance.\n",
    "\n",
    "7. **Consider Ensemble Methods:**\n",
    "   - Evaluate ensemble methods like Random Forests, Gradient Boosting, or stacking. Ensemble methods often provide better generalization performance by combining the predictions of multiple base models.\n",
    "\n",
    "8. **Check Model Assumptions:**\n",
    "   - Ensure that the chosen model aligns with the assumptions of your data. For instance, linear models assume a linear relationship between features and the target variable.\n",
    "\n",
    "9. **Perform Model Selection Techniques:**\n",
    "   - Use techniques such as grid search or random search for hyperparameter tuning. These techniques help you find the optimal combination of hyperparameters for a given model.\n",
    "\n",
    "10. **Evaluate Computational Resources:**\n",
    "    - Consider the computational resources required for training and deploying the model. Some models, especially deep neural networks, may demand significant computational power.\n",
    "\n",
    "11. **Assess Interpretability:**\n",
    "    - If interpretability is crucial for your application or if you need to explain predictions to stakeholders, choose a model that offers interpretability, such as decision trees or linear models.\n",
    "\n",
    "12. **Experiment with Deep Learning (if applicable):**\n",
    "    - For complex tasks with large amounts of data, consider experimenting with deep learning models, such as neural networks. Deep learning excels in tasks like image recognition, natural language processing, and complex pattern recognition.\n",
    "\n",
    "13. **Evaluate Transfer Learning (if applicable):**\n",
    "    - If you have limited labeled data, consider using transfer learning with pre-trained models. Transfer learning can leverage knowledge gained from one task and apply it to a related task.\n",
    "\n",
    "14. **Consider Model Explainability (if applicable):**\n",
    "    - If interpretability is essential, explore models that provide explanations for their predictions, such as LIME (Local Interpretable Model-agnostic Explanations) or SHAP (SHapley Additive exPlanations).\n",
    "\n",
    "15. **Check for Model Libraries/Frameworks:**\n",
    "    - Evaluate available machine learning libraries or frameworks that support the chosen model. Consider factors like community support, documentation, and ease of integration.\n",
    "\n",
    "16. **Perform Model Robustness Analysis:**\n",
    "    - Assess the robustness of the chosen model to variations in the data. Evaluate how well the model generalizes to unseen data and handles potential outliers or noisy inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-8 Train the Model\n",
    "Training the model is a crucial step in the machine learning pipeline. Here are the sub-tasks involved in training a machine learning model:\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - Ensure that the training data is properly preprocessed and formatted according to the requirements of the chosen model. Features and labels should be in a compatible format.\n",
    "\n",
    "2. **Split the Data:**\n",
    "   - Use the previously defined training set to train the model. If you're using a validation set for hyperparameter tuning, set aside the validation set for this phase.\n",
    "\n",
    "3. **Select Hyperparameters:**\n",
    "   - Choose the hyperparameters of the model. This may involve manually selecting values or using techniques like grid search or random search to find the best hyperparameter combination.\n",
    "\n",
    "4. **Initialize the Model:**\n",
    "   - Instantiate the model using the selected hyperparameters. This involves setting up the architecture and parameters of the model.\n",
    "\n",
    "5. **Loss Function Selection:**\n",
    "   - Define the loss function appropriate for your task. The loss function measures the difference between the model's predictions and the actual values and guides the learning process.\n",
    "\n",
    "6. **Optimization Algorithm Selection:**\n",
    "   - Choose an optimization algorithm to minimize the loss function during training. Common optimization algorithms include stochastic gradient descent (SGD), Adam, and RMSprop.\n",
    "\n",
    "7. **Model Training:**\n",
    "   - Train the model on the training set by feeding input features and corresponding labels through the network. The model adjusts its parameters during this process to minimize the chosen loss function.\n",
    "\n",
    "8. **Monitor Training Progress:**\n",
    "   - Monitor the model's performance on both the training and validation sets during training. Plot learning curves to observe how the loss changes over epochs or iterations.\n",
    "\n",
    "9. **Early Stopping (if applicable):**\n",
    "   - Implement early stopping to halt training when the model's performance on the validation set stops improving. This prevents overfitting and saves computation resources.\n",
    "\n",
    "10. **Regularization Techniques (if applicable):**\n",
    "    - Apply regularization techniques, such as L1 or L2 regularization, dropout, or batch normalization, to prevent overfitting and improve the model's generalization performance.\n",
    "\n",
    "11. **Hyperparameter Tuning (if applicable):**\n",
    "    - Fine-tune hyperparameters based on the model's performance on the validation set. Adjust parameters like learning rate, dropout rate, or the number of hidden units in neural networks.\n",
    "\n",
    "12. **Model Evaluation on Validation Set:**\n",
    "    - Assess the model's performance on the validation set using appropriate evaluation metrics. This helps understand how well the model generalizes to unseen data.\n",
    "\n",
    "13. **Iterative Adjustments:**\n",
    "    - Make iterative adjustments to the model architecture, hyperparameters, or preprocessing steps based on insights gained from training and validation results.\n",
    "\n",
    "14. **Final Model Training:**\n",
    "    - Once satisfied with the model's performance on the validation set, train the model on the entire training dataset (including the validation set if it was used for hyperparameter tuning) to maximize its learning potential.\n",
    "\n",
    "15. **Evaluate on Test Set:**\n",
    "    - Assess the model's performance on the test set, which it has not seen during training. This provides a reliable estimate of the model's generalization to new, unseen data.\n",
    "\n",
    "16. **Model Interpretability (if applicable):**\n",
    "    - If interpretability is essential, use techniques or tools to interpret and explain the model's decisions. This is particularly important for models deployed in sensitive or regulated domains.\n",
    "\n",
    "17. **Save Model and Parameters:**\n",
    "    - Save the trained model and its parameters to be used for making predictions on new data or for deployment in a production environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-9 Evaluate the Model\n",
    "Evaluating the model is a critical step in the machine learning process to assess its performance and generalization capabilities. Here are the sub-tasks involved in evaluating a machine learning model:\n",
    "\n",
    "1. **Prepare Test Data:**\n",
    "   - Ensure that the test data is properly preprocessed and formatted, similar to the training data, so it can be used for model evaluation.\n",
    "\n",
    "2. **Load Trained Model:**\n",
    "   - Load the trained model, including its architecture and learned parameters, into memory.\n",
    "\n",
    "3. **Make Predictions:**\n",
    "   - Use the trained model to make predictions on the test set or new, unseen data. This involves passing input features through the model and obtaining corresponding predictions.\n",
    "\n",
    "4. **Evaluate Predictions:**\n",
    "   - Compare the model's predictions with the actual target values in the test set. The evaluation metrics used depend on the nature of the problem (classification, regression, etc.).\n",
    "\n",
    "5. **Compute Performance Metrics:**\n",
    "   - Calculate relevant performance metrics such as accuracy, precision, recall, F1 score, mean squared error, or other appropriate metrics based on the problem type. These metrics provide insights into the model's behavior.\n",
    "\n",
    "6. **Confusion Matrix (for Classification):**\n",
    "   - Create a confusion matrix to visualize the model's performance in a classification task. This matrix displays true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "7. **ROC Curve and AUC (for Classification):**\n",
    "   - For binary classification tasks, plot the Receiver Operating Characteristic (ROC) curve and calculate the Area Under the Curve (AUC) to assess the model's ability to discriminate between classes.\n",
    "\n",
    "8. **Precision-Recall Curve (for Classification):**\n",
    "   - Plot the precision-recall curve to visualize the trade-off between precision and recall at different decision thresholds for binary classification problems.\n",
    "\n",
    "9. **Regression Diagnostics (for Regression):**\n",
    "   - Analyze residual plots, such as a scatter plot of predicted vs. actual values, to assess the performance of regression models. Look for patterns or trends that might indicate model deficiencies.\n",
    "\n",
    "10. **Cross-Validation Evaluation (optional):**\n",
    "    - If cross-validation was not performed during training, consider using cross-validation on the test set for a more robust evaluation. This helps assess the model's stability across different data splits.\n",
    "\n",
    "11. **Performance Comparison:**\n",
    "    - Compare the model's performance with baseline models or other models that might be considered for the task. This provides context and helps determine whether the chosen model is suitable.\n",
    "\n",
    "12. **Interpretability Analysis (if applicable):**\n",
    "    - If interpretability is important, use interpretability tools or techniques to understand how the model is making decisions. This is crucial for models deployed in sensitive or regulated domains.\n",
    "\n",
    "13. **Error Analysis:**\n",
    "    - Analyze errors made by the model on the test set. Identify patterns or specific cases where the model performs poorly, and consider whether additional data or model improvements can address these issues.\n",
    "\n",
    "14. **Consider Business Metrics (if applicable):**\n",
    "    - If the model is deployed in a business context, consider additional business metrics beyond standard machine learning metrics. These might include cost-benefit analysis, return on investment, or other business-specific key performance indicators.\n",
    "\n",
    "15. **Model Robustness Testing:**\n",
    "    - Evaluate the model's robustness by testing its performance on data distributions that may differ from the training distribution. This helps ensure that the model generalizes well to real-world scenarios.\n",
    "\n",
    "16. **Threshold Tuning (for Classification):**\n",
    "    - Experiment with decision thresholds, especially in binary classification tasks, to find a balance between precision and recall that aligns with the application's requirements.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
